{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从任意的 LLM 推理提供商中创建一个 Transformers 智能体\n",
    "_作者: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
    "\n",
    "> 本教程建立在智能体知识的基础上：要了解更多关于智能体的信息，你可以从[这里介绍](agents)开始。\n",
    "\n",
    "[Transformers Agents](https://huggingface.co/docs/transformers/en/agents) 是一个用于构建智能体的库，它使用 LLM 在 `llm_engine` 参数中提供动力。这个参数的设计是为了给用户最大的自由度去选择任意 LLM。\n",
    "\n",
    "让我们看看如何从一些主要提供商的 API 中构建这个 `llm_engine`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace 无服务器 API 和专用端点\n",
    "\n",
    "Transformers Agents 提供了一个内置的 `HfEngine` 类，允许你通过无服务器 API 或你自己的专用端点使用 Hub 上的任何模型。这是使用 HF 智能体的首选方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mWhat's the 10th Fibonacci number?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unicodedata', 're', 'math', 'collections', 'queue', 'itertools', 'random', 'time', 'stat', 'statistics']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m0\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m1\u001b[39m\n",
      "\u001b[38;5;109;01mfor\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7m_\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01min\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;109mrange\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;139m9\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[38;5;7m:\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m55\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m0\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m1\u001b[39m\n",
      "\u001b[38;5;109;01mfor\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7m_\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01min\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;109mrange\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;139m9\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[38;5;7m:\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7ma\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mb\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m55\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.agents import HfEngine, ReactCodeAgent\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "endpoint_url = \"your_endpoint_url\"\n",
    "\n",
    "llm_engine = HfEngine(model=repo_id)  # you could use model=endpoint_url here\n",
    "\n",
    "agent = ReactCodeAgent(tools=[], llm_engine=llm_engine)\n",
    "\n",
    "agent.run(\"What's the 10th Fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的 `llm_engine` 初始化参数可以是一个简单的可调用对象，如下所示：\n",
    "```py\n",
    "def llm_engine(messages, stop_sequences=[]) -> str:\n",
    "    return response(messages)\n",
    "```\n",
    "这个可调用对象是 llm 引擎的核心。它应该满足以下要求：\n",
    "- 以 [聊天模板](https://huggingface.co/docs/transformers/main/en/chat_templating) 格式的消息列表作为输入，并输出一个 `str`。\n",
    "- 接受一个 `stop_sequences` 参数，智能体系统将传递给它应该停止生成的序列。\n",
    "\n",
    "让我们更仔细地看看我们使用的 `HfEngine` 的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from transformers.agents.llm_engine import MessageRole, get_clean_message_list\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "llama_role_conversions = {\n",
    "    MessageRole.TOOL_RESPONSE: MessageRole.USER,\n",
    "}\n",
    "\n",
    "\n",
    "class HfEngine:\n",
    "    def __init__(self, model: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        self.model = model\n",
    "        self.client = InferenceClient(model=self.model, timeout=120)\n",
    "\n",
    "    def __call__(self, messages: List[Dict[str, str]], stop_sequences=[]) -> str:\n",
    "        # Get clean message list\n",
    "        messages = get_clean_message_list(\n",
    "            messages, role_conversions=llama_role_conversions\n",
    "        )\n",
    "\n",
    "        # Get LLM output\n",
    "        response = self.client.chat_completion(\n",
    "            messages, stop=stop_sequences, max_tokens=1500\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "\n",
    "        # Remove stop sequences from LLM output\n",
    "        for stop_seq in stop_sequences:\n",
    "            if response[-len(stop_seq) :] == stop_seq:\n",
    "                response = response[: -len(stop_seq)]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，引擎不是一个函数，而是一个带有 `__call__` 方法的类，这使得存储诸如客户端之类的属性成为可能。\n",
    "\n",
    "我们还使用了 `get_clean_message_list()` 实用工具来将连续的消息连接到同一个角色。\n",
    "这个方法接受一个 `role_conversions` 参数，用于将 Transformers 智能体支持的角色的范围转换为你的 LLM 所接受的那些角色。\n",
    "\n",
    "这个配方可以适用于任何 LLM！让我们看看其他例子。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为任何 LLM 适配配方\n",
    "\n",
    "使用上述配方，你可以使用任何 LLM 推理源作为你的 `llm_engine`。\n",
    "只需记住两个主要约束：\n",
    "- `llm_engine` 是一个可调用对象，它以 [聊天模板](https://huggingface.co/docs/transformers/main/en/chat_templating) 格式的消息列表作为输入，并输出一个 `str`。\n",
    "- 它接受一个 `stop_sequences` 参数。\n",
    "\n",
    "\n",
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_role_conversions = {\n",
    "    MessageRole.TOOL_RESPONSE: MessageRole.USER,\n",
    "}\n",
    "\n",
    "\n",
    "class OpenAIEngine:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "\n",
    "    def __call__(self, messages, stop_sequences=[]):\n",
    "        messages = get_clean_message_list(\n",
    "            messages, role_conversions=openai_role_conversions\n",
    "        )\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            stop=stop_sequences,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic, AnthropicBedrock\n",
    "\n",
    "\n",
    "# Cf this page for using Anthropic from Bedrock: https://docs.anthropic.com/en/api/claude-on-amazon-bedrock\n",
    "class AnthropicEngine:\n",
    "    def __init__(self, model_name=\"claude-3-5-sonnet-20240620\", use_bedrock=False):\n",
    "        self.model_name = model_name\n",
    "        if use_bedrock:\n",
    "            self.model_name = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "            self.client = AnthropicBedrock(\n",
    "                aws_access_key=os.getenv(\"AWS_BEDROCK_ID\"),\n",
    "                aws_secret_key=os.getenv(\"AWS_BEDROCK_KEY\"),\n",
    "                aws_region=\"us-east-1\",\n",
    "            )\n",
    "        else:\n",
    "            self.client = Anthropic(\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            )\n",
    "\n",
    "    def __call__(self, messages, stop_sequences=[]):\n",
    "        messages = get_clean_message_list(\n",
    "            messages, role_conversions=openai_role_conversions\n",
    "        )\n",
    "        index_system_message, system_prompt = None, None\n",
    "        for index, message in enumerate(messages):\n",
    "            if message[\"role\"] == MessageRole.SYSTEM:\n",
    "                index_system_message = index\n",
    "                system_prompt = message[\"content\"]\n",
    "        if system_prompt is None:\n",
    "            raise Exception(\"No system prompt found!\")\n",
    "\n",
    "        filtered_messages = [\n",
    "            message for i, message in enumerate(messages) if i != index_system_message\n",
    "        ]\n",
    "        if len(filtered_messages) == 0:\n",
    "            print(\"Error, no user message:\", messages)\n",
    "            assert False\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            system=system_prompt,\n",
    "            messages=filtered_messages,\n",
    "            stop_sequences=stop_sequences,\n",
    "            temperature=0.5,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        full_response_text = \"\"\n",
    "        for content_block in response.content:\n",
    "            if content_block.type == \"text\":\n",
    "                full_response_text += content_block.text\n",
    "        return full_response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下一步\n",
    "\n",
    "现在去为你自己选择的那个语言模型推理服务，用 `transformers.agents` 做一个 `llm_engine` 吧！\n",
    "\n",
    "做好之后，你可以用这个新的 `llm_engine` 来玩玩这些应用场景：\n",
    "- [智能体 RAG：通过查询重构和自查询来增强你的 RAG ](agent_rag)\n",
    "- [用于文本到 SQL 的智能体，带自动错误校正](agent_text_to_sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disposable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
